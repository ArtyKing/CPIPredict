{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0384a9d1-936c-415c-9c62-805506dbdd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration: a=0.103, b=1.0000, sigma(train)=86.454\n",
      "\n",
      "=== OUTPUTS written to ./outputs ===\n",
      "\n",
      "               MAE_level  RMSE_level\n",
      "Model                               \n",
      "GrandEnsemble   0.747666    0.856851\n",
      "SARIMAX_exog    0.963528    1.115070\n",
      "KNN             9.853637   11.006709\n",
      "XGB_or_GBR      9.720245   11.063773\n",
      "RandomForest   13.400760   14.916692\n",
      "\n",
      "MoM metrics:\n",
      "                 MAE_MoM  RMSE_MoM\n",
      "Model                            \n",
      "GrandEnsemble  0.001195  0.001509\n",
      "SARIMAX_exog   0.001216  0.001530\n",
      "KNN            0.002408  0.002980\n",
      "RandomForest   0.003047  0.003480\n",
      "XGB_or_GBR     0.003113  0.004259\n",
      "\n",
      "YoY metrics:\n",
      "                 MAE_YoY  RMSE_YoY\n",
      "Model                            \n",
      "GrandEnsemble  0.001857  0.002227\n",
      "SARIMAX_exog   0.002119  0.002608\n",
      "KNN            0.026840  0.026952\n",
      "XGB_or_GBR     0.028820  0.029124\n",
      "RandomForest   0.036930  0.036982\n",
      "\n",
      "QoQ SAAR metrics:\n",
      "                MAE_QoQ_SAAR  RMSE_QoQ_SAAR\n",
      "Model                                     \n",
      "GrandEnsemble      0.010161       0.012140\n",
      "SARIMAX_exog       0.010253       0.012184\n",
      "KNN                0.027598       0.030114\n",
      "XGB_or_GBR         0.031772       0.038208\n",
      "RandomForest       0.036681       0.039450\n",
      "\n",
      "Special features in TRAIN: Import=0, Tariff=1776, TariffWgt=0\n",
      "Special features in TEST:  Import=0,  Tariff=1776,  TariffWgt=0\n",
      "\n",
      "Top import/tariff categories (by permutation or proxy importance):\n",
      "                      permutation_importance\n",
      "tariff_hs2_50_lag7             -5.617729e-14\n",
      "tariff_hs2_50_lag8             -7.627232e-14\n",
      "tariff_hs2_50_lag5             -7.671641e-14\n",
      "tariff_hs2_50_roll3            -8.093526e-14\n",
      "tariff_hs2_50_roll12           -8.781864e-14\n",
      "tariff_hs2_66_roll12           -8.903989e-14\n",
      "tariff_hs2_50_lag4             -8.937295e-14\n",
      "tariff_hs2_50_lag6             -9.214851e-14\n",
      "tariff_hs2_50_lag11            -9.436896e-14\n",
      "tariff_hs2_50_lag9             -9.581225e-14\n"
     ]
    }
   ],
   "source": [
    "# cpi_pipeline.py\n",
    "# CPI prediction & presentation pipeline with Import-weighted tariff features and robust calibration.\n",
    "# (Fixes: robust HS2 column detection in Import.xlsx; datetime column headers; HS2 padding to 2 digits.)\n",
    "\n",
    "import os, io, re, glob, json, zipfile, argparse, warnings\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# XGBoost (fallback to GBR if missing)\n",
    "XGB_PRESENT = True\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "except Exception:\n",
    "    XGB_PRESENT = False\n",
    "    from sklearn.ensemble import GradientBoostingRegressor as XGBRegressor\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "\n",
    "# ---------------- CLI ----------------\n",
    "def parse_args(argv=None):\n",
    "    p = argparse.ArgumentParser(description=\"CPI Nowcasting & Presentation Pipeline\")\n",
    "    p.add_argument(\"--base_dir\", default=os.environ.get(\"BASE_DIR\", \".\"), help=\"Folder with CPI.csv etc.\")\n",
    "    p.add_argument(\"--out_dir\", default=None, help=\"Output directory (default: <base_dir>/outputs)\")\n",
    "    p.add_argument(\"--auto_sarima\", action=\"store_true\", help=\"Small SARIMA grid by AIC.\")\n",
    "    p.add_argument(\"--test_months\", type=int, default=24, help=\"Test split length (months).\")\n",
    "    p.add_argument(\"--coverage\", type=float, default=0.80, help=\"Min coverage for most features.\")\n",
    "    p.add_argument(\"--protected_cov\", type=float, default=0.50, help=\"Lower coverage for Import__/tariff_/TariffWgt__.\")\n",
    "    p.add_argument(\"--min_nonnull_import_tariff\", type=int, default=6, help=\"Min non-nulls to keep Import__/tariff_*/TariffWgt__.\")\n",
    "    p.add_argument(\"--pi_max_features\", type=int, default=60, help=\"Max features for permutation importance.\")\n",
    "    p.add_argument(\"--target_mom\", action=\"store_true\", help=\"Train ML on CPI MoM and reconstruct Level.\")\n",
    "    p.add_argument(\"--model_start\", default=\"2010-01\", help=\"First month for the modeling window.\")\n",
    "    args, _ = p.parse_known_args(argv)  # ignore Jupyter -f\n",
    "    return args\n",
    "\n",
    "\n",
    "# ---------------- I/O helpers ----------------\n",
    "def parse_date_col(df: pd.DataFrame) -> pd.Series:\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            parsed = pd.to_datetime(df[col], errors=\"coerce\", infer_datetime_format=True)\n",
    "            if parsed.notna().mean() > 0.7:\n",
    "                return parsed\n",
    "        except Exception:\n",
    "            pass\n",
    "    idx = pd.to_datetime(df.index, errors=\"coerce\", infer_datetime_format=True)\n",
    "    if idx.notna().mean() > 0.7:\n",
    "        return idx\n",
    "    raise ValueError(\"Could not infer a date column.\")\n",
    "\n",
    "def try_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in out.columns:\n",
    "        if out[c].dtype == \"O\":\n",
    "            out[c] = (out[c].astype(str)\n",
    "                      .str.replace(\",\", \"\", regex=False)\n",
    "                      .str.replace(\"%\", \"\", regex=False))\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "    out = out.loc[:, out.notna().any(axis=0)]\n",
    "    return out\n",
    "\n",
    "def robust_read(path: str) -> Dict[str, pd.DataFrame]:\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    out: Dict[str, pd.DataFrame] = {}\n",
    "    if ext == \".csv\":\n",
    "        try:\n",
    "            out[\"main\"] = pd.read_csv(path)\n",
    "        except UnicodeDecodeError:\n",
    "            out[\"main\"] = pd.read_csv(path, encoding=\"latin-1\")\n",
    "    elif ext == \".xlsx\":\n",
    "        x = pd.ExcelFile(path)  # openpyxl\n",
    "        for s in x.sheet_names:\n",
    "            try: out[s] = x.parse(s)\n",
    "            except Exception: pass\n",
    "    elif ext == \".xls\":\n",
    "        try:\n",
    "            import xlrd  # noqa\n",
    "            x = pd.ExcelFile(path, engine=\"xlrd\")\n",
    "            for s in x.sheet_names:\n",
    "                try: out[s] = x.parse(s)\n",
    "                except Exception: pass\n",
    "        except Exception:\n",
    "            return {}\n",
    "    else:\n",
    "        return {}\n",
    "    return out\n",
    "\n",
    "\n",
    "# -------------- wide/long monthly unifiers --------------\n",
    "def unify_monthly_series(dfs: Dict[str, pd.DataFrame], prefer_sum: bool = False) -> pd.DataFrame:\n",
    "    frames = []\n",
    "    for name, df in dfs.items():\n",
    "        try:\n",
    "            date_idx = parse_date_col(df)\n",
    "            df = df.copy(); df.index = pd.to_datetime(date_idx)\n",
    "            drop_candidates = [c for c in df.columns if any(k in str(c).lower() for k in [\"date\", \"time\", \"period\", \"month\"])]\n",
    "            df = df.drop(columns=drop_candidates, errors=\"ignore\")\n",
    "            df = try_numeric(df).sort_index()\n",
    "            agg = \"sum\" if prefer_sum else \"mean\"\n",
    "            m = df.resample(\"MS\").agg(agg)\n",
    "            m.columns = [f\"{name}__{c}\" for c in m.columns]\n",
    "            frames.append(m)\n",
    "        except Exception:\n",
    "            continue\n",
    "    if not frames: return pd.DataFrame()\n",
    "    out = pd.concat(frames, axis=1).sort_index()\n",
    "    return out.groupby(level=0, axis=1).first()\n",
    "\n",
    "def _date_like_cols(cols):\n",
    "    parsed = []\n",
    "    for c in cols:\n",
    "        try:\n",
    "            dt = pd.to_datetime(str(c), errors=\"coerce\")\n",
    "            if pd.notna(dt):\n",
    "                ms = pd.Period(dt, freq=\"M\").to_timestamp('M') + pd.offsets.MonthBegin(1)\n",
    "                parsed.append((c, ms))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return parsed\n",
    "\n",
    "def melt_wide_months(df: pd.DataFrame, prefer_sum=False) -> pd.DataFrame:\n",
    "    candidates = _date_like_cols(df.columns)\n",
    "    if len(candidates) < 6:  # need several months at least\n",
    "        return pd.DataFrame()\n",
    "    col_map = {orig: m for orig, m in candidates}\n",
    "    tmp = try_numeric(df[list(col_map.keys())].copy())\n",
    "    agg = np.nansum if prefer_sum else np.nanmean\n",
    "    vec = agg(tmp.values, axis=0)\n",
    "    ser = pd.Series(vec, index=[col_map[c] for c in tmp.columns], name=\"value\").sort_index()\n",
    "    return ser.resample(\"MS\").mean().to_frame()\n",
    "\n",
    "def unify_monthly_any_format(dfs: Dict[str, pd.DataFrame], prefer_sum=False, prefix=None) -> pd.DataFrame:\n",
    "    frames = []\n",
    "    for name, df in dfs.items():\n",
    "        got = unify_monthly_series({name: df}, prefer_sum=prefer_sum)\n",
    "        if got.empty or got.shape[0] <= 2:\n",
    "            m = melt_wide_months(df, prefer_sum=prefer_sum)\n",
    "            if not m.empty:\n",
    "                m.columns = [f\"{name}__{c}\" for c in m.columns]\n",
    "                frames.append(m)\n",
    "        else:\n",
    "            frames.append(got)\n",
    "    if not frames: return pd.DataFrame()\n",
    "    out = pd.concat(frames, axis=1).sort_index()\n",
    "    out = out.groupby(level=0, axis=1).first()\n",
    "    if prefix: out = out.add_prefix(prefix)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------------- Tariff ZIP reader ----------------\n",
    "def read_tariff_zip(zip_path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(zip_path): return pd.DataFrame()\n",
    "    monthly_list, hs2_list = [], []\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        for member in z.namelist():\n",
    "            if not member.lower().endswith((\".csv\", \".xlsx\", \".xls\")): continue\n",
    "            with z.open(member) as f: data = f.read()\n",
    "            buf = io.BytesIO(data)\n",
    "\n",
    "            df = None\n",
    "            if member.lower().endswith(\".csv\"):\n",
    "                for enc in [None, \"latin-1\"]:\n",
    "                    try:\n",
    "                        df = pd.read_csv(buf if enc is None else io.BytesIO(data),\n",
    "                                         encoding=None if enc is None else \"latin-1\")\n",
    "                        break\n",
    "                    except Exception: df = None\n",
    "            elif member.lower().endswith(\".xlsx\"):\n",
    "                try: df = pd.read_excel(buf)\n",
    "                except Exception: df = None\n",
    "            elif member.lower().endswith(\".xls\"):\n",
    "                try:\n",
    "                    import xlrd  # noqa\n",
    "                    df = pd.read_excel(buf, engine=\"xlrd\")\n",
    "                except Exception: df = None\n",
    "            if df is None or df.empty: continue\n",
    "\n",
    "            # Date\n",
    "            date_col = None\n",
    "            for c in df.columns:\n",
    "                if any(k in str(c).lower() for k in [\"date\", \"period\", \"month\"]):\n",
    "                    parsed = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "                    if parsed.notna().mean() > 0.5: date_col = c; break\n",
    "            if date_col is None:\n",
    "                idx = pd.to_datetime(df.index, errors=\"coerce\")\n",
    "                if idx.notna().mean() > 0.5: df = df.copy(); df.index = idx\n",
    "                else: continue\n",
    "            else:\n",
    "                df = df.copy(); df.index = pd.to_datetime(df[date_col]); df = df.drop(columns=[date_col], errors=\"ignore\")\n",
    "\n",
    "            # Rate columns\n",
    "            cand = [c for c in df.columns if any(k in str(c).lower() for k in [\"tariff\", \"duty\", \"rate\", \"advalorem\"])]\n",
    "            if not cand: continue\n",
    "            numeric = try_numeric(df[cand])\n",
    "            rates = numeric.mean(axis=1, skipna=True)\n",
    "            monthly_list.append(rates.resample(\"MS\").mean().rename(\"tariff_overall\"))\n",
    "\n",
    "            # HS2 split\n",
    "            hs_col = None\n",
    "            for c in df.columns:\n",
    "                if re.search(r\"\\b(HS|HTS).*(CODE)?\\b\", str(c), re.I): hs_col = c; break\n",
    "            if hs_col is None:\n",
    "                for c in df.columns:\n",
    "                    if \"code\" in str(c).lower(): hs_col = c; break\n",
    "            if hs_col is not None:\n",
    "                tmp = pd.DataFrame({\n",
    "                    \"date\": df.index,\n",
    "                    \"hs\": df[hs_col].astype(str).str.extract(r\"(\\d{1,2})\", expand=False).str.zfill(2),\n",
    "                    \"rate\": rates.values,\n",
    "                }).dropna(subset=[\"hs\"])\n",
    "                tmp[\"date\"] = pd.to_datetime(tmp[\"date\"])\n",
    "                grp = tmp.groupby([pd.Grouper(key=\"date\", freq=\"MS\"), \"hs\"])[\"rate\"].mean().reset_index()\n",
    "                piv = grp.pivot(index=\"date\", columns=\"hs\", values=\"rate\").add_prefix(\"tariff_hs2_\")\n",
    "                hs2_list.append(piv)\n",
    "\n",
    "    out = None\n",
    "    if monthly_list:\n",
    "        out = pd.concat(monthly_list, axis=1).mean(axis=1).to_frame(\"tariff_overall\")\n",
    "    if hs2_list:\n",
    "        hs2 = pd.concat(hs2_list, axis=1).groupby(level=0, axis=1).mean()\n",
    "        out = hs2.join(out, how=\"outer\") if out is not None else hs2\n",
    "    return out if out is not None else pd.DataFrame()\n",
    "\n",
    "\n",
    "# ------------- Import.xlsx → HS2 monthly values -------------\n",
    "def _extract_hs_col(df: pd.DataFrame) -> Optional[str]:\n",
    "    # FIX: always treat column names as strings\n",
    "    for c in df.columns:\n",
    "        cl = str(c).lower()\n",
    "        if re.search(r\"\\b(hs|hts).*code\\b\", cl) or cl in {\"hs\", \"hts\", \"code\", \"commodity code\", \"hs code\", \"hts code\", \"chapter\"}:\n",
    "            return c\n",
    "    for c in df.columns:\n",
    "        if \"code\" in str(c).lower() or str(c).lower() in {\"hs2\", \"hs_2\", \"chapter\"}:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def read_import_hs2(import_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Return monthly HS2 import values: columns like imp_hs2_01..imp_hs2_99 and Import__TOTAL.\"\"\"\n",
    "    if not os.path.exists(import_path): return pd.DataFrame()\n",
    "    dfs = robust_read(import_path)\n",
    "    frames = []\n",
    "    for name, df in dfs.items():\n",
    "        if df is None or df.empty: continue\n",
    "        hs_col = _extract_hs_col(df)\n",
    "        if hs_col:\n",
    "            # WIDE case: rows are HS, columns are months (many datetime-like headers)\n",
    "            date_cols = _date_like_cols(df.columns)\n",
    "            if date_cols:\n",
    "                col_map = {orig: m for orig, m in date_cols}\n",
    "                cols = [hs_col] + list(col_map.keys())\n",
    "                tmp = df[cols].copy()\n",
    "                tmp.columns = [hs_col] + [col_map[c] for c in col_map.keys()]\n",
    "                tmp = try_numeric(tmp)\n",
    "                tmp[\"hs2\"] = (tmp[hs_col].astype(str)\n",
    "                              .str.extract(r\"(\\d{1,2})\", expand=False)\n",
    "                              .str.zfill(2))\n",
    "                tmp = tmp.dropna(subset=[\"hs2\"])\n",
    "                melt = tmp.drop(columns=[hs_col]).melt(id_vars=\"hs2\", var_name=\"date\", value_name=\"value\")\n",
    "                melt[\"date\"] = pd.to_datetime(melt[\"date\"])\n",
    "                grp = melt.groupby([pd.Grouper(key=\"date\", freq=\"MS\"), \"hs2\"])[\"value\"].sum().reset_index()\n",
    "                piv = grp.pivot(index=\"date\", columns=\"hs2\", values=\"value\").add_prefix(\"imp_hs2_\")\n",
    "                frames.append(piv)\n",
    "                continue\n",
    "            # LONG case: hs/date/value in columns\n",
    "            maybe_date = None\n",
    "            for c in df.columns:\n",
    "                if any(k in str(c).lower() for k in [\"date\", \"period\", \"month\"]):\n",
    "                    maybe_date = c; break\n",
    "            val_cols = [c for c in df.columns if c not in {hs_col, maybe_date} and df[c].dtype != \"O\"]\n",
    "            if maybe_date and val_cols:\n",
    "                tmp = df[[hs_col, maybe_date] + val_cols].copy()\n",
    "                tmp[\"hs2\"] = (tmp[hs_col].astype(str)\n",
    "                              .str.extract(r\"(\\d{1,2})\", expand=False)\n",
    "                              .str.zfill(2))\n",
    "                tmp[maybe_date] = pd.to_datetime(tmp[maybe_date])\n",
    "                tmp[\"value\"] = try_numeric(tmp[val_cols]).sum(axis=1, min_count=1)\n",
    "                grp = tmp.groupby([pd.Grouper(key=maybe_date, freq=\"MS\"), \"hs2\"])[\"value\"].sum().reset_index()\n",
    "                piv = grp.pivot(index=maybe_date, columns=\"hs2\", values=\"value\").add_prefix(\"imp_hs2_\")\n",
    "                frames.append(piv)\n",
    "                continue\n",
    "        # Fallback: if sheet has only totals by month\n",
    "        m = unify_monthly_any_format({name: df}, prefer_sum=True)\n",
    "        if not m.empty:\n",
    "            m.columns = [c if str(c).startswith(\"Import__\") else f\"Import__{name}__{c}\" for c in m.columns]\n",
    "            frames.append(m)\n",
    "    if not frames: return pd.DataFrame()\n",
    "    imp = pd.concat(frames, axis=1).sort_index()\n",
    "    # collapse duplicated columns and build TOTAL\n",
    "    imp = imp.groupby(level=0, axis=1).sum(min_count=1)\n",
    "    hs2_cols = [c for c in imp.columns if c.startswith(\"imp_hs2_\")]\n",
    "    if hs2_cols:\n",
    "        imp[\"Import__TOTAL\"] = imp[hs2_cols].sum(axis=1, min_count=1)\n",
    "    elif \"Import__TOTAL\" not in imp.columns:\n",
    "        imp[\"Import__TOTAL\"] = imp.sum(axis=1, min_count=1)\n",
    "    return imp\n",
    "\n",
    "\n",
    "def build_import_weighted_tariffs(tariffs: pd.DataFrame, imp_hs2: pd.DataFrame, topn: int = 12) -> pd.DataFrame:\n",
    "    \"\"\"Create TariffWgt__Index and top-N HS2 weighted contributions: share(HSt) * tariff(HSt).\"\"\"\n",
    "    if tariffs is None or tariffs.empty or imp_hs2 is None or imp_hs2.empty:\n",
    "        return pd.DataFrame()\n",
    "    hs2_tar_cols = [c for c in tariffs.columns if re.match(r\"tariff_hs2_\\d{2}$\", c)]\n",
    "    hs2_imp_cols = [c for c in imp_hs2.columns if re.match(r\"imp_hs2_\\d{2}$\", c)]\n",
    "    if not hs2_tar_cols or not hs2_imp_cols:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Align to monthly MS frequency\n",
    "    t = tariffs[hs2_tar_cols].copy().asfreq(\"MS\")\n",
    "    m = imp_hs2[hs2_imp_cols + ([\"Import__TOTAL\"] if \"Import__TOTAL\" in imp_hs2.columns else [])].copy().asfreq(\"MS\")\n",
    "    if \"Import__TOTAL\" not in m.columns:\n",
    "        m[\"Import__TOTAL\"] = m[hs2_imp_cols].sum(axis=1, min_count=1)\n",
    "\n",
    "    # Shares\n",
    "    shares = m[hs2_imp_cols].div(m[\"Import__TOTAL\"].replace(0, np.nan), axis=0)\n",
    "    shares = shares.fillna(0.0)\n",
    "\n",
    "    # Pairs in common\n",
    "    def pair_name(c_imp): return c_imp.replace(\"imp_\", \"tariff_\")\n",
    "    imp_common = [c for c in hs2_imp_cols if pair_name(c) in t.columns]\n",
    "    if not imp_common:\n",
    "        return pd.DataFrame()\n",
    "    tar_common = [pair_name(c) for c in imp_common]\n",
    "\n",
    "    # Weighted index\n",
    "    w_contribs = shares[imp_common].values * t[tar_common].values\n",
    "    index = pd.Series(np.nansum(w_contribs, axis=1), index=t.index, name=\"TariffWgt__Index\")\n",
    "\n",
    "    # Top-N HS2 by average share\n",
    "    avg_share = shares[imp_common].mean().sort_values(ascending=False)\n",
    "    top_imp_cols = list(avg_share.head(topn).index)\n",
    "    top_tar_cols = [pair_name(c) for c in top_imp_cols]\n",
    "    top_w = pd.DataFrame(shares[top_imp_cols].values * t[top_tar_cols].values, index=t.index,\n",
    "                         columns=[f\"TariffWgt__{c[-2:]}\" for c in top_imp_cols])\n",
    "\n",
    "    out = pd.concat([index, top_w], axis=1)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------------- Metrics & transforms ----------------\n",
    "def annualize_qoq_saar(last3_mom: List[float]) -> float:\n",
    "    try: return float(np.prod([1 + m for m in last3_mom])**4 - 1.0)\n",
    "    except Exception: return np.nan\n",
    "\n",
    "def compute_qoq_saar_series(level_series: pd.Series) -> pd.Series:\n",
    "    mom = level_series.pct_change()\n",
    "    out = []\n",
    "    for i in range(len(level_series)):\n",
    "        if i < 3 or mom.iloc[i-2:i+1].isna().any(): out.append(np.nan)\n",
    "        else: out.append(annualize_qoq_saar(mom.iloc[i-2:i+1].values))\n",
    "    return pd.Series(out, index=level_series.index, name=f\"{level_series.name}_QoQ_SAAR\")\n",
    "\n",
    "def rmse(a, b): \n",
    "    return float(mean_squared_error(a, b) ** 0.5)\n",
    "\n",
    "\n",
    "# ---------------- Data assembly & features ----------------\n",
    "def build_feature_table(base_dir: str, model_start: str) -> Tuple[pd.Series, pd.DataFrame, Dict]:\n",
    "    files = {\n",
    "        \"cpi\": os.path.join(base_dir, \"CPI.csv\"),\n",
    "        \"usd\": os.path.join(base_dir, \"USD Index.csv\"),\n",
    "        \"rent\": os.path.join(base_dir, \"Rent Price.csv\"),\n",
    "        \"house\": os.path.join(base_dir, \"Housing Price.csv\"),\n",
    "        \"oil\": os.path.join(base_dir, \"Oil.xls\"),\n",
    "        \"freight\": os.path.join(base_dir, \"Freight.xlsx\"),\n",
    "        \"import\": os.path.join(base_dir, \"Import.xlsx\"),\n",
    "    }\n",
    "    zip_paths = sorted(set(glob.glob(os.path.join(base_dir, \"tariff*.zip\")) +\n",
    "                           glob.glob(os.path.join(base_dir, \"tariff_data_*.zip\"))))\n",
    "\n",
    "    audit = {\"base_dir\": base_dir, \"found_files\": {}, \"notes\": [], \"tariff_zips_used\": []}\n",
    "\n",
    "    def read_any(path, prefer_sum=False, label=None, prefix=None):\n",
    "        label = label or os.path.basename(path)\n",
    "        exists = os.path.exists(path)\n",
    "        audit[\"found_files\"][label] = {\"exists\": exists, \"path\": path}\n",
    "        if not exists: return pd.DataFrame()\n",
    "        m = unify_monthly_any_format(robust_read(path), prefer_sum=prefer_sum, prefix=prefix)\n",
    "        audit[\"found_files\"][label][\"shape\"] = list(m.shape) if m is not None else None\n",
    "        return m\n",
    "\n",
    "    # Target\n",
    "    cpi_df = unify_monthly_any_format(robust_read(files[\"cpi\"]), prefer_sum=False)\n",
    "    cpi_cols = [c for c in cpi_df.columns if cpi_df[c].notna().sum() > 10]\n",
    "    if not cpi_cols: raise RuntimeError(\"CPI.csv had no usable numeric columns.\")\n",
    "    y = cpi_df[cpi_cols[0]].rename(\"CPI\")\n",
    "\n",
    "    # Exogenous (macro/freight/etc.)\n",
    "    feats = []\n",
    "    feats.append(read_any(files[\"usd\"],    False, \"USD Index.csv\",     \"USD__\"))\n",
    "    feats.append(read_any(files[\"rent\"],   False, \"Rent Price.csv\",    \"Rent__\"))\n",
    "    feats.append(read_any(files[\"house\"],  False, \"Housing Price.csv\", \"House__\"))\n",
    "    feats.append(read_any(files[\"oil\"],    False, \"Oil.xls\",           \"Oil__\"))\n",
    "    feats.append(read_any(files[\"freight\"],False, \"Freight.xlsx\",      \"Freight__\"))\n",
    "\n",
    "    # Imports & tariffs\n",
    "    imp_wide = read_any(files[\"import\"], True, \"Import.xlsx\", \"Import__\")  # totals (if any)\n",
    "    imp_hs2 = read_import_hs2(files[\"import\"])  # HS2 values\n",
    "    if not imp_wide.empty and \"Import__AGG\" not in imp_wide.columns:\n",
    "        imp_wide[\"Import__AGG\"] = imp_wide.filter(regex=r\"^Import__\").sum(axis=1, min_count=1)\n",
    "    tariffs_list = []\n",
    "    for zp in zip_paths:\n",
    "        t = read_tariff_zip(zp); lab = os.path.basename(zp)\n",
    "        audit[\"found_files\"][lab] = {\"exists\": os.path.exists(zp), \"path\": zp, \"shape\": list(t.shape) if t is not None else None}\n",
    "        if not t.empty: tariffs_list.append(t); audit[\"tariff_zips_used\"].append(lab)\n",
    "    tariffs = pd.concat(tariffs_list, axis=1).groupby(level=0, axis=1).mean() if tariffs_list else pd.DataFrame()\n",
    "\n",
    "    # Import-weighted tariffs\n",
    "    tariff_wgt = build_import_weighted_tariffs(tariffs, imp_hs2, topn=12)\n",
    "\n",
    "    # Merge exogenous blocks\n",
    "    X = None\n",
    "    for block in [imp_wide, imp_hs2, tariffs, tariff_wgt] + feats:\n",
    "        if block is None or block.empty: continue\n",
    "        X = block if X is None else X.join(block, how=\"outer\")\n",
    "\n",
    "    if X is None or X.empty: raise RuntimeError(\"No exogenous features parsed.\")\n",
    "\n",
    "    # Align monthly & trim\n",
    "    df = pd.concat([y, X], axis=1).sort_index()\n",
    "    df = df[~df.index.duplicated(keep=\"first\")].asfreq(\"MS\").ffill().dropna(thresh=2)\n",
    "    start_ts = pd.to_datetime(model_start) if model_start else df.index.min()\n",
    "    df = df.loc[df.index >= start_ts]\n",
    "\n",
    "    audit[\"target_span\"] = [str(df.index.min().date()), str(df.index.max().date())]\n",
    "    exog_raw = df.drop(columns=[\"CPI\"])\n",
    "    audit[\"exog_columns_raw_count\"] = exog_raw.shape[1]\n",
    "    audit[\"exog_span\"] = [str(df.index.min().date()), str(df.index.max().date())]\n",
    "    audit[\"import_weighted_tariff_present\"] = bool(tariff_wgt is not None and not tariff_wgt.empty)\n",
    "\n",
    "    return df[\"CPI\"], exog_raw, audit\n",
    "\n",
    "\n",
    "def engineer_features(y: pd.Series, X: pd.DataFrame,\n",
    "                      coverage: float = 0.80, protected_cov: float = 0.50,\n",
    "                      min_nonnull_import_tariff: int = 6):\n",
    "    \"\"\"Numeric-only exog; protect Import__/tariff_/TariffWgt__; replace ±∞→NaN; transforms+lags (incl. tariffs).\"\"\"\n",
    "    X_num = X.select_dtypes(include=[np.number]).copy()\n",
    "    df = pd.concat([y.rename(\"CPI\"), X_num], axis=1).sort_index()\n",
    "\n",
    "    cov = X_num.notna().mean(); nn = X_num.notna().sum()\n",
    "    is_special = (X_num.columns.str.startswith(\"Import__\")\n",
    "                  | X_num.columns.str.contains(\"tariff_\", case=False)\n",
    "                  | X_num.columns.str.startswith(\"TariffWgt__\"))\n",
    "\n",
    "    keep_general   = cov[cov >= coverage].index.tolist()\n",
    "    keep_protected = cov[is_special & ((cov >= protected_cov) | (nn >= min_nonnull_import_tariff))].index.tolist()\n",
    "    exog_keep = sorted(set(keep_general) | set(keep_protected))\n",
    "    if not any(col.startswith(\"Import__\") for col in exog_keep):\n",
    "        exog_keep += [c for c in X_num.columns if c == \"Import__AGG\"]\n",
    "\n",
    "    Xg = X_num[exog_keep].copy()\n",
    "\n",
    "    # Target transforms\n",
    "    df[\"CPI_mom\"] = df[\"CPI\"].pct_change()\n",
    "    df[\"CPI_yoy\"] = df[\"CPI\"].pct_change(12)\n",
    "\n",
    "    # Exog transforms\n",
    "    for c in exog_keep:\n",
    "        s = Xg[c]\n",
    "        df[f\"{c}_mom\"]    = s.pct_change()\n",
    "        df[f\"{c}_yoy\"]    = s.pct_change(12)\n",
    "        df[f\"{c}_roll3\"]  = s.rolling(3).mean()\n",
    "        df[f\"{c}_roll6\"]  = s.rolling(6).mean()\n",
    "        df[f\"{c}_roll12\"] = s.rolling(12).mean()\n",
    "\n",
    "    # Replace ±∞ -> NaN\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # Lags (CPI + key channels incl. tariffs & TariffWgt__)\n",
    "    key_exog = [c for c in exog_keep\n",
    "                if c.startswith((\"Freight__\", \"Oil__\", \"Import__\", \"USD__\", \"TariffWgt__\"))\n",
    "                or (\"tariff_\" in c.lower())]\n",
    "    base_cols = [\"CPI\", \"CPI_mom\", \"CPI_yoy\"] + [c for c in key_exog if c in df.columns]\n",
    "    for c in base_cols:\n",
    "        for L in range(1, 13):\n",
    "            df[f\"{c}_lag{L}\"] = df[c].shift(L)\n",
    "\n",
    "    # Coverage pass & fill\n",
    "    cov2 = df.notna().mean(); nn2 = df.notna().sum()\n",
    "    idx_series = cov2.index.to_series()\n",
    "    is_prot2 = (idx_series.str.startswith(\"Import__\")\n",
    "                | idx_series.str.contains(\"tariff_\", case=False)\n",
    "                | idx_series.str.startswith(\"TariffWgt__\"))\n",
    "    keep_cols = cov2[(cov2 >= coverage)\n",
    "                     | (is_prot2 & ((cov2 >= protected_cov) | (nn2 >= min_nonnull_import_tariff)))].index\n",
    "    df = df[keep_cols].ffill().bfill()\n",
    "\n",
    "    y_ml = df[\"CPI\"].copy()\n",
    "    X_ml = df.drop(columns=[\"CPI\"])\n",
    "\n",
    "    # Compact exog for SARIMAX\n",
    "    sarimax_exog = None\n",
    "    candidates = [c for c in df.columns if c.startswith(\"Freight__\") and (c.endswith(\"_mom\") or c.endswith(\"_roll3\") or c.endswith(\"_roll6\"))]\n",
    "    if candidates: sarimax_exog = df[[candidates[0]]].copy()\n",
    "\n",
    "    prov = {\n",
    "        \"exog_used_count\": X_ml.shape[1],\n",
    "        \"exog_used_sample\": X_ml.columns[:20].tolist(),\n",
    "        \"exog_import_cols_count\": int(sum(X_ml.columns.to_series().str.startswith(\"Import__\"))),\n",
    "        \"exog_tariff_cols_count\": int(sum(X_ml.columns.to_series().str.contains(\"tariff_\", case=False))),\n",
    "        \"exog_tariffwgt_cols_count\": int(sum(X_ml.columns.to_series().str.startswith(\"TariffWgt__\")))\n",
    "    }\n",
    "    return y_ml, X_ml, sarimax_exog, prov\n",
    "\n",
    "\n",
    "def train_test_split_time(X: pd.DataFrame, y: pd.Series, test_months: int = 24):\n",
    "    if len(y) < test_months + 36:\n",
    "        test_months = max(12, min(18, len(y)//4))\n",
    "    split_point = y.index[-test_months]\n",
    "    X_tr, X_te = X.loc[:split_point - pd.offsets.MonthBegin(0)], X.loc[split_point:]\n",
    "    y_tr, y_te = y.loc[:split_point - pd.offsets.MonthBegin(0)], y.loc[split_point:]\n",
    "    return X_tr, X_te, y_tr, y_te\n",
    "\n",
    "\n",
    "# ---------------- Modeling ----------------\n",
    "def fit_sarima(y_train: pd.Series, auto: bool = False):\n",
    "    if not auto:\n",
    "        order, sorder = (0, 1, 1), (0, 1, 1, 12)\n",
    "        res = SARIMAX(y_train, order=order, seasonal_order=sorder,\n",
    "                      enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "        return (order, sorder), res\n",
    "\n",
    "    best_aic = np.inf; best = None; best_res = None\n",
    "    for p in [0, 1, 2]:\n",
    "        for q in [0, 1, 2]:\n",
    "            for P in [0, 1]:\n",
    "                for Q in [0, 1]:\n",
    "                    try:\n",
    "                        order, sorder = (p, 1, q), (P, 1, Q, 12)\n",
    "                        res = SARIMAX(y_train, order=order, seasonal_order=sorder,\n",
    "                                      enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "                        if res.aic < best_aic:\n",
    "                            best_aic, best, best_res = res.aic, (order, sorder), res\n",
    "                    except Exception: pass\n",
    "    if best_res is None: return fit_sarima(y_train, auto=False)\n",
    "    return best, best_res\n",
    "\n",
    "def train_model_zoo():\n",
    "    models = {\n",
    "        \"XGB_or_GBR\": XGBRegressor(\n",
    "            n_estimators=600 if XGB_PRESENT else 400,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9 if XGB_PRESENT else None,\n",
    "            random_state=42,\n",
    "            tree_method=\"hist\" if XGB_PRESENT else None,\n",
    "            enable_categorical=False\n",
    "        ),\n",
    "        \"RandomForest\": RandomForestRegressor(n_estimators=500, max_depth=8, random_state=42, n_jobs=-1),\n",
    "        \"KNN\": make_pipeline(StandardScaler(with_mean=False), KNeighborsRegressor(n_neighbors=5, weights=\"distance\"))\n",
    "    }\n",
    "    return models\n",
    "\n",
    "def _sanitize_xy(X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    Xc = X.replace([np.inf, -np.inf], np.nan)\n",
    "    yc = y.replace([np.inf, -np.inf], np.nan)\n",
    "    Xc = Xc.fillna(Xc.mean()); yc = yc.fillna(yc.mean())\n",
    "    return Xc.astype(np.float64), yc.astype(np.float64)\n",
    "\n",
    "def fit_predict_zoo(models: Dict[str, object], X_train, y_train, X_test) -> Dict[str, pd.Series]:\n",
    "    out = {}\n",
    "    X_tr, y_tr = _sanitize_xy(X_train, y_train)\n",
    "    X_te = X_test.replace([np.inf, -np.inf], np.nan).fillna(X_tr.mean()).astype(np.float64)\n",
    "    for name, m in models.items():\n",
    "        m.fit(X_tr, y_tr)\n",
    "        out[name] = pd.Series(m.predict(X_te), index=X_test.index, name=f\"{name}_pred\")\n",
    "    return out\n",
    "\n",
    "def refine_sarimax(y_train, y_test, sarimax_exog):\n",
    "    if sarimax_exog is None: return None, None\n",
    "    exog_all = sarimax_exog.reindex(pd.concat([y_train, y_test]).index).replace([np.inf, -np.inf], np.nan).ffill()\n",
    "    exog_tr = exog_all.loc[y_train.index]; exog_te = exog_all.loc[y_test.index]\n",
    "    try:\n",
    "        res = SARIMAX(y_train, order=(1,1,1), seasonal_order=(0,1,1,12),\n",
    "                      exog=exog_tr, enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "        fc_te = res.get_forecast(steps=len(y_test), exog=exog_te).predicted_mean\n",
    "        # In-sample predicted mean aligned with y_train\n",
    "        fc_tr = res.get_prediction(start=y_train.index[0], end=y_train.index[-1]).predicted_mean\n",
    "        return pd.Series(fc_tr, index=y_train.index, name=\"SARIMAX_exog_in\"), pd.Series(fc_te, index=y_test.index, name=\"SARIMAX_exog_pred\")\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# -------------- Calibration & CIs --------------\n",
    "def robust_sigma(resid: pd.Series) -> float:\n",
    "    resid = resid.dropna()\n",
    "    if len(resid) == 0: return 0.0\n",
    "    mad = np.median(np.abs(resid - np.median(resid)))\n",
    "    return float(max(resid.std(ddof=1), 1.4826 * mad))  # conservative\n",
    "\n",
    "def calibrate_bias(y_train: pd.Series,\n",
    "                   sarima_train_pred: pd.Series,\n",
    "                   sarimax_train_pred: Optional[pd.Series],\n",
    "                   zoo_train_mean: Optional[pd.Series],\n",
    "                   weights: dict,\n",
    "                   window: int = 12):\n",
    "    w_sarima  = float(weights.get(\"SARIMA\", 0.0))\n",
    "    w_sarimax = float(weights.get(\"SARIMAX_exog\", 0.0))\n",
    "    w_zoo     = float(sum(v for k, v in weights.items() if k not in [\"SARIMA\", \"SARIMAX_exog\"]))\n",
    "\n",
    "    # Compose train-side ensemble using the *same* weights\n",
    "    grand_train = (w_sarima * sarima_train_pred.reindex(y_train.index)).fillna(0.0)\n",
    "    if sarimax_train_pred is not None:\n",
    "        grand_train = grand_train.add(w_sarimax * sarimax_train_pred.reindex(y_train.index), fill_value=0.0)\n",
    "    if zoo_train_mean is not None and not zoo_train_mean.empty:\n",
    "        grand_train = grand_train.add(w_zoo * zoo_train_mean.reindex(y_train.index), fill_value=0.0)\n",
    "\n",
    "    idx = y_train.dropna().index.intersection(grand_train.dropna().index)\n",
    "    if len(idx) < max(6, window): window = max(6, len(idx)//2 or 6)\n",
    "    tail = idx[-window:]\n",
    "\n",
    "    # OLS slope + intercept\n",
    "    X = grand_train.loc[tail].values.reshape(-1, 1)\n",
    "    y = y_train.loc[tail].values\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X, y)\n",
    "    a_raw = float(lr.intercept_); b_raw = float(lr.coef_[0])\n",
    "\n",
    "    # Guardrails: clamp slope or fallback to intercept-only if unstable\n",
    "    if not np.isfinite(b_raw) or b_raw < 0.7 or b_raw > 1.3:\n",
    "        b = 1.0\n",
    "        a = float(np.median(y - X.ravel()))  # robust mean shift\n",
    "    else:\n",
    "        a, b = a_raw, b_raw\n",
    "\n",
    "    grand_adj = a + b * grand_train\n",
    "    sigma = robust_sigma(y_train.loc[tail] - grand_adj.loc[tail])\n",
    "    return a, b, grand_adj, sigma\n",
    "\n",
    "\n",
    "# -------------- Evaluation & plotting --------------\n",
    "def compute_metrics(y_test: pd.Series, pred_map: Dict[str, Optional[pd.Series]]):\n",
    "    # Level\n",
    "    rows = []\n",
    "    for name, p in pred_map.items():\n",
    "        if p is None: continue\n",
    "        idx = y_test.dropna().index.intersection(p.dropna().index)\n",
    "        if len(idx) < 1: continue\n",
    "        a, b = y_test.loc[idx], p.loc[idx]\n",
    "        rows.append({\"Model\": name, \"MAE_level\": mean_absolute_error(a, b), \"RMSE_level\": rmse(a, b)})\n",
    "    level = pd.DataFrame(rows).set_index(\"Model\").sort_values(\"RMSE_level\") if rows else pd.DataFrame(columns=[\"MAE_level\",\"RMSE_level\"])\n",
    "\n",
    "    # MoM\n",
    "    actual_mom = y_test.pct_change()\n",
    "    rows = []\n",
    "    for name, p in pred_map.items():\n",
    "        if p is None: continue\n",
    "        m = p.pct_change()\n",
    "        idx = actual_mom.dropna().index.intersection(m.dropna().index)\n",
    "        if len(idx) < 1: continue\n",
    "        a, b = actual_mom.loc[idx], m.loc[idx]\n",
    "        rows.append({\"Model\": name, \"MAE_MoM\": mean_absolute_error(a, b), \"RMSE_MoM\": rmse(a, b)})\n",
    "    mom = pd.DataFrame(rows).set_index(\"Model\").sort_values(\"RMSE_MoM\") if rows else pd.DataFrame(columns=[\"MAE_MoM\",\"RMSE_MoM\"])\n",
    "\n",
    "    # YoY\n",
    "    actual_yoy = y_test.pct_change(12)\n",
    "    rows = []\n",
    "    for name, p in pred_map.items():\n",
    "        if p is None: continue\n",
    "        m = p.pct_change(12)\n",
    "        idx = actual_yoy.dropna().index.intersection(m.dropna().index)\n",
    "        if len(idx) < 1: continue\n",
    "        a, b = actual_yoy.loc[idx], m.loc[idx]\n",
    "        rows.append({\"Model\": name, \"MAE_YoY\": mean_absolute_error(a, b), \"RMSE_YoY\": rmse(a, b)})\n",
    "    yoy = pd.DataFrame(rows).set_index(\"Model\").sort_values(\"RMSE_YoY\") if rows else pd.DataFrame(columns=[\"MAE_YoY\",\"RMSE_YoY\"])\n",
    "\n",
    "    # QoQ SAAR\n",
    "    actual_qoq = compute_qoq_saar_series(y_test)\n",
    "    rows = []\n",
    "    for name, p in pred_map.items():\n",
    "        if p is None: continue\n",
    "        q = compute_qoq_saar_series(p)\n",
    "        idx = actual_qoq.dropna().index.intersection(q.dropna().index)\n",
    "        if len(idx) < 1: continue\n",
    "        a, b = actual_qoq.loc[idx], q.loc[idx]\n",
    "        rows.append({\"Model\": name, \"MAE_QoQ_SAAR\": mean_absolute_error(a, b), \"RMSE_QoQ_SAAR\": rmse(a, b)})\n",
    "    qoq = pd.DataFrame(rows).set_index(\"Model\").sort_values(\"RMSE_QoQ_SAAR\") if rows else pd.DataFrame(columns=[\"MAE_QoQ_SAAR\",\"RMSE_QoQ_SAAR\"])\n",
    "\n",
    "    return level, mom, yoy, qoq\n",
    "\n",
    "def save_top_features_bar(series: pd.Series, path: str, title: str, topn: int = 20):\n",
    "    vals = series.sort_values(ascending=False).head(topn)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    vals[::-1].plot(kind=\"barh\")\n",
    "    plt.title(title); plt.tight_layout(); plt.savefig(path, dpi=150); plt.close()\n",
    "\n",
    "def rolling_rmse(a: pd.Series, b: pd.Series, window=6) -> pd.Series:\n",
    "    err = (a - b)**2\n",
    "    r = err.rolling(window).mean()**0.5\n",
    "    return r\n",
    "\n",
    "def pretty_plot(dates, actual, pred, title, ylabel, path, ci95=None, ci75=None):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(dates, actual, label=\"Actual\")\n",
    "    plt.plot(dates, pred, label=\"Predicted\", linestyle=\"--\")\n",
    "    if ci95 is not None:\n",
    "        plt.fill_between(dates, ci95[0], ci95[1], alpha=0.18, label=\"95% CI\")\n",
    "    if ci75 is not None:\n",
    "        plt.fill_between(dates, ci75[0], ci75[1], alpha=0.18, label=\"75% CI\")\n",
    "    plt.title(title); plt.ylabel(ylabel); plt.xlabel(\"\")\n",
    "    plt.legend(); plt.tight_layout(); plt.savefig(path, dpi=150); plt.close()\n",
    "\n",
    "\n",
    "# ---------------- Main ----------------\n",
    "def main(args=None):\n",
    "    if args is None:\n",
    "        args = parse_args()\n",
    "\n",
    "    base_dir = args.base_dir\n",
    "    out_dir = args.out_dir or os.path.join(base_dir, \"outputs\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # 1) Data\n",
    "    y, X_raw, audit = build_feature_table(base_dir, args.model_start)\n",
    "    X = X_raw.select_dtypes(include=[np.number]).copy()\n",
    "    audit[\"exog_columns_numeric_count\"] = X.shape[1]\n",
    "    audit[\"exog_columns_non_numeric_dropped\"] = sorted(list(set(X_raw.columns) - set(X.columns)))\n",
    "\n",
    "    # 2) Features\n",
    "    y_ml, X_ml, sarimax_exog, feat_prov = engineer_features(\n",
    "        y, X, coverage=args.coverage, protected_cov=args.protected_cov,\n",
    "        min_nonnull_import_tariff=args.min_nonnull_import_tariff\n",
    "    )\n",
    "    audit.update(feat_prov)\n",
    "\n",
    "    # Optional: ML on MoM\n",
    "    use_mom = bool(args.target_mom)\n",
    "    if use_mom:\n",
    "        y_mom = y.pct_change().replace([np.inf, -np.inf], np.nan)\n",
    "        X_lag1 = X.shift(1).replace([np.inf, -np.inf], np.nan)\n",
    "        data = pd.concat([y_mom, X_lag1], axis=1).dropna()\n",
    "        y_ml = data.iloc[:, 0].rename(\"CPI_mom\")\n",
    "        X_ml = data.iloc[:, 1:]\n",
    "\n",
    "    # Save engineered tables\n",
    "    try:\n",
    "        y_ml.to_csv(os.path.join(out_dir, \"y_ml.csv\"))\n",
    "        X_ml.to_parquet(os.path.join(out_dir, \"X_ml.parquet\"))\n",
    "    except Exception:\n",
    "        X_ml.to_csv(os.path.join(out_dir, \"X_ml.csv\"))\n",
    "\n",
    "    # 3) Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split_time(X_ml, y_ml, test_months=args.test_months)\n",
    "\n",
    "    # Audit: presence of special features\n",
    "    audit[\"import_cols_in_train\"]    = int(sum(X_train.columns.to_series().str.startswith(\"Import__\")))\n",
    "    audit[\"tariff_cols_in_train\"]    = int(sum(X_train.columns.to_series().str.contains(\"tariff_\", case=False)))\n",
    "    audit[\"tariffwgt_cols_in_train\"] = int(sum(X_train.columns.to_series().str.startswith(\"TariffWgt__\")))\n",
    "    audit[\"import_cols_in_test\"]     = int(sum(X_test.columns.to_series().str.startswith(\"Import__\")))\n",
    "    audit[\"tariff_cols_in_test\"]     = int(sum(X_test.columns.to_series().str.contains(\"tariff_\", case=False)))\n",
    "    audit[\"tariffwgt_cols_in_test\"]  = int(sum(X_test.columns.to_series().str.startswith(\"TariffWgt__\")))\n",
    "\n",
    "    # 4) SARIMA on level y (train window)\n",
    "    (order, sorder), sarima_res = fit_sarima(\n",
    "        y.loc[y.index <= y_test.index[-1] - pd.offsets.MonthBegin(0)],\n",
    "        auto=args.auto_sarima\n",
    "    )\n",
    "    sarima_fc = sarima_res.get_forecast(steps=len(y_test))\n",
    "    sarima_pred_level = pd.Series(sarima_fc.predicted_mean, index=y_test.index, name=\"SARIMA_pred\")\n",
    "    # In-sample predicted mean (not fittedvalues to avoid differencing quirks)\n",
    "    sarima_train_pred = sarima_res.get_prediction(start=y_train.index[0], end=y_train.index[-1]).predicted_mean\n",
    "    sarima_train_pred = pd.Series(sarima_train_pred, index=y_train.index, name=\"SARIMA_in\")\n",
    "\n",
    "    # 5) Model zoo\n",
    "    models = train_model_zoo()\n",
    "    zoo_preds = fit_predict_zoo(models, X_train, y_train, X_test)\n",
    "\n",
    "    # If ML target is MoM, reconstruct to LEVEL\n",
    "    if use_mom:\n",
    "        start_level = y.loc[y.index < y_test.index[0]].iloc[-1]\n",
    "        def mom_to_level(mom_series: pd.Series, start_val: float) -> pd.Series:\n",
    "            out, cur = [], float(start_val)\n",
    "            for g in mom_series.fillna(0).values:\n",
    "                cur *= (1 + float(g)); out.append(cur)\n",
    "            return pd.Series(out, index=mom_series.index)\n",
    "        for k in list(zoo_preds.keys()):\n",
    "            zoo_preds[k] = mom_to_level(zoo_preds[k], start_level).rename(zoo_preds[k].name.replace(\"_pred\", \"_level_pred\"))\n",
    "        y_test_level = y.loc[y_test.index]\n",
    "    else:\n",
    "        y_test_level = y_test\n",
    "\n",
    "    # 6) SARIMAX refinement (train+test)\n",
    "    sarimax_train_in, sarimax_pred = refine_sarimax(y.loc[y.index < y_test.index[0]], y.loc[y_test.index], sarimax_exog)\n",
    "\n",
    "    # 7) Weighted ensemble\n",
    "    preds_dict = {\"SARIMA\": sarima_pred_level, \"SARIMAX_exog\": sarimax_pred}\n",
    "    preds_dict.update(zoo_preds)\n",
    "\n",
    "    perf = {}\n",
    "    for name, p in preds_dict.items():\n",
    "        if p is None: continue\n",
    "        idx = y_test_level.index.intersection(p.dropna().index)\n",
    "        if len(idx) < 1: continue\n",
    "        perf[name] = rmse(y_test_level.loc[idx], p.loc[idx])\n",
    "\n",
    "    weights = {k: 1.0 / (v**2 + 1e-9) for k, v in perf.items()}\n",
    "    s = sum(weights.values())\n",
    "    if s == 0 or not weights: weights = {\"SARIMA\": 1.0}\n",
    "    else: weights = {k: v/s for k, v in weights.items()}\n",
    "\n",
    "    aligned = []\n",
    "    for name, p in preds_dict.items():\n",
    "        if p is None or name not in weights: continue\n",
    "        aligned.append(weights[name] * p.reindex(y_test_level.index))\n",
    "    grand_ensemble = (pd.concat(aligned, axis=1).sum(axis=1) if aligned\n",
    "                      else sarima_pred_level.copy()).rename(\"GrandEnsemble_pred\")\n",
    "\n",
    "    # 8) Metrics (Level/MoM/YoY/QoQ SAAR)\n",
    "    metrics_inputs = {\"SARIMA\": sarima_pred_level, \"SARIMAX_exog\": sarimax_pred, **zoo_preds, \"GrandEnsemble\": grand_ensemble}\n",
    "    level_metrics, mom_metrics, yoy_metrics, qoq_metrics = compute_metrics(y_test_level, metrics_inputs)\n",
    "\n",
    "    # 9) Calibration & CIs (train-tail)\n",
    "    # Train-side zoo preds\n",
    "    zoo_train_preds = {}\n",
    "    X_tr_san, y_tr_san = _sanitize_xy(X_train, y_train)\n",
    "    for name, m in train_model_zoo().items():\n",
    "        m.fit(X_tr_san, y_tr_san)\n",
    "        zoo_train_preds[name] = pd.Series(m.predict(X_tr_san), index=X_train.index)\n",
    "    if use_mom:\n",
    "        seed = y.loc[y.index < X_train.index[0]].iloc[-1]\n",
    "        for k, srs in zoo_train_preds.items():\n",
    "            cur, vals = float(seed), []\n",
    "            for g in srs.fillna(0).values:\n",
    "                cur *= (1 + float(g)); vals.append(cur)\n",
    "            zoo_train_preds[k] = pd.Series(vals, index=X_train.index)\n",
    "    avg_zoo_train = (pd.concat(zoo_train_preds.values(), axis=1).mean(axis=1)\n",
    "                     if zoo_train_preds else pd.Series(index=y.index, dtype=float))\n",
    "\n",
    "    a_cal, b_cal, grand_train_adj, sigma_train = calibrate_bias(\n",
    "        y.loc[y_train.index],\n",
    "        sarima_train_pred,\n",
    "        sarimax_train_in,\n",
    "        avg_zoo_train,\n",
    "        weights,\n",
    "        window=12\n",
    "    )\n",
    "    pred_raw = grand_ensemble\n",
    "    pred_adj = a_cal + b_cal * pred_raw\n",
    "    z95, z75 = 1.96, 1.150349\n",
    "    ci95_lo = pred_adj - z95 * sigma_train; ci95_hi = pred_adj + z95 * sigma_train\n",
    "    ci75_lo = pred_adj - z75 * sigma_train; ci75_hi = pred_adj + z75 * sigma_train\n",
    "\n",
    "    print(f\"Calibration: a={a_cal:.3f}, b={b_cal:.4f}, sigma(train)={sigma_train:.3f}\")\n",
    "\n",
    "    # 10) Main charts — Level + YoY%\n",
    "    lvl_path = os.path.join(out_dir, \"cpi_level_pred_vs_actual.png\")\n",
    "    pretty_plot(y_test_level.index, y_test_level, pred_adj,\n",
    "                \"CPI Level: Actual vs Predicted (Grand Ensemble, calibrated)\",\n",
    "                \"CPI Index (1982-84=100)\", lvl_path, ci95=(ci95_lo, ci95_hi), ci75=(ci75_lo, ci75_hi))\n",
    "\n",
    "    denom12 = y.shift(12).reindex(pred_adj.index)\n",
    "    yoy_actual = (y.reindex(pred_adj.index) / denom12 - 1.0) * 100.0\n",
    "    yoy_pred   = (pred_adj / denom12 - 1.0) * 100.0\n",
    "    yoy_ci95_lo = (ci95_lo / denom12 - 1.0) * 100.0\n",
    "    yoy_ci95_hi = (ci95_hi / denom12 - 1.0) * 100.0\n",
    "    yoy_ci75_lo = (ci75_lo / denom12 - 1.0) * 100.0\n",
    "    yoy_ci75_hi = (ci75_hi / denom12 - 1.0) * 100.0\n",
    "    mask = denom12.notna() & y_test_level.reindex(pred_adj.index).notna()\n",
    "    dates = pred_adj.index[mask]\n",
    "    yoy_path = os.path.join(out_dir, \"cpi_yoy_pred_vs_actual.png\")\n",
    "    pretty_plot(dates, yoy_actual.loc[dates], yoy_pred.loc[dates],\n",
    "                \"CPI YoY%: Actual vs Predicted (Grand Ensemble, calibrated)\",\n",
    "                \"YoY (%)\", yoy_path,\n",
    "                ci95=(yoy_ci95_lo.loc[dates], yoy_ci95_hi.loc[dates]),\n",
    "                ci75=(yoy_ci75_lo.loc[dates], yoy_ci75_hi.loc[dates]))\n",
    "\n",
    "    # 11) Additional presentation charts\n",
    "    # MoM%\n",
    "    denom1 = y.shift(1).reindex(pred_adj.index)\n",
    "    mom_actual = (y.reindex(pred_adj.index) / denom1 - 1.0) * 100.0\n",
    "    mom_pred   = (pred_adj / denom1 - 1.0) * 100.0\n",
    "    mom_ci95_lo = (ci95_lo / denom1 - 1.0) * 100.0\n",
    "    mom_ci95_hi = (ci95_hi / denom1 - 1.0) * 100.0\n",
    "    mom_ci75_lo = (ci75_lo / denom1 - 1.0) * 100.0\n",
    "    mom_ci75_hi = (ci75_hi / denom1 - 1.0) * 100.0\n",
    "    mask_mom = denom1.notna() & y_test_level.reindex(pred_adj.index).notna()\n",
    "    dates_mom = pred_adj.index[mask_mom]\n",
    "    mom_path = os.path.join(out_dir, \"cpi_mom_pred_vs_actual.png\")\n",
    "    pretty_plot(dates_mom, mom_actual.loc[dates_mom], mom_pred.loc[dates_mom],\n",
    "                \"CPI MoM%: Actual vs Predicted (calibrated)\", \"MoM (%)\", mom_path,\n",
    "                ci95=(mom_ci95_lo.loc[dates_mom], mom_ci95_hi.loc[dates_mom]),\n",
    "                ci75=(mom_ci75_lo.loc[dates_mom], mom_ci75_hi.loc[dates_mom]))\n",
    "\n",
    "    # QoQ SAAR\n",
    "    qoq_actual = compute_qoq_saar_series(y.reindex(pred_adj.index))\n",
    "    qoq_pred   = compute_qoq_saar_series(pred_adj)\n",
    "    qoq_ci95_lo = compute_qoq_saar_series(ci95_lo)\n",
    "    qoq_ci95_hi = compute_qoq_saar_series(ci95_hi)\n",
    "    qoq_ci75_lo = compute_qoq_saar_series(ci75_lo)\n",
    "    qoq_ci75_hi = compute_qoq_saar_series(ci75_hi)\n",
    "    mask_qoq = qoq_actual.notna() & qoq_pred.notna()\n",
    "    dates_qoq = qoq_pred.index[mask_qoq]\n",
    "    qoq_path = os.path.join(out_dir, \"cpi_qoq_saar_pred_vs_actual.png\")\n",
    "    pretty_plot(dates_qoq, (qoq_actual*100).loc[dates_qoq], (qoq_pred*100).loc[dates_qoq],\n",
    "                \"CPI QoQ SAAR: Actual vs Predicted (calibrated)\", \"QoQ SAAR (%)\", qoq_path,\n",
    "                ci95=((qoq_ci95_lo*100).loc[dates_qoq], (qoq_ci95_hi*100).loc[dates_qoq]),\n",
    "                ci75=((qoq_ci75_lo*100).loc[dates_qoq], (qoq_ci75_hi*100).loc[dates_qoq]))\n",
    "\n",
    "    # Residuals & scatter (level)\n",
    "    resid = (y_test_level - pred_adj).dropna()\n",
    "    plt.figure(figsize=(10,4)); plt.plot(resid.index, resid.values); plt.title(\"Residuals (Actual - Pred, Level)\"); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"residuals_level.png\"), dpi=150); plt.close()\n",
    "    plt.figure(figsize=(5,5)); plt.scatter(y_test_level, pred_adj); \n",
    "    mn, mx = float(min(y_test_level.min(), pred_adj.min())), float(max(y_test_level.max(), pred_adj.max()))\n",
    "    plt.plot([mn, mx], [mn, mx], linestyle=\"--\"); plt.title(\"Actual vs Predicted (Level)\"); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"scatter_level.png\"), dpi=150); plt.close()\n",
    "\n",
    "    # Rolling RMSE (6m)\n",
    "    rr = rolling_rmse(y_test_level, pred_adj, window=6)\n",
    "    plt.figure(figsize=(10,4)); plt.plot(rr.index, rr.values); plt.title(\"Rolling RMSE (6m, Level)\"); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"rolling_rmse_6m.png\"), dpi=150); plt.close()\n",
    "\n",
    "    # 12) Train metrics\n",
    "    train_rows = []\n",
    "    for name, m in train_model_zoo().items():\n",
    "        m.fit(X_tr_san, y_tr_san)\n",
    "        p = pd.Series(m.predict(X_tr_san), index=X_train.index)\n",
    "        train_rows.append({\"Model\": name, \"Train_MAE\": mean_absolute_error(y.loc[X_train.index], p),\n",
    "                           \"Train_RMSE\": rmse(y.loc[X_train.index], p)})\n",
    "    sarima_in = sarima_train_pred  # already aligned\n",
    "    train_rows.append({\"Model\": \"SARIMA\", \"Train_MAE\": mean_absolute_error(y.loc[sarima_in.index], sarima_in),\n",
    "                       \"Train_RMSE\": rmse(y.loc[sarima_in.index], sarima_in)})\n",
    "    train_metrics = pd.DataFrame(train_rows).set_index(\"Model\").sort_values(\"Train_RMSE\")\n",
    "\n",
    "    # 13) Feature importance\n",
    "    best_name = level_metrics.index[0] if not level_metrics.empty else \"RandomForest\"\n",
    "    model_for_imp = train_model_zoo().get(best_name, train_model_zoo().get(\"RandomForest\"))\n",
    "    corr_abs = X_tr_san.corrwith(y_tr_san).abs().sort_values(ascending=False)\n",
    "    top_cols = corr_abs.index[:max(5, args.pi_max_features)]\n",
    "    try:\n",
    "        model_for_imp.fit(X_tr_san[top_cols], y_tr_san)\n",
    "        X_te_san = X_test.replace([np.inf, -np.inf], np.nan).fillna(X_tr_san[top_cols].mean()).astype(np.float64)\n",
    "        imp = permutation_importance(model_for_imp, X_te_san[top_cols], y_test, n_repeats=8, random_state=42, n_jobs=-1)\n",
    "        imp_vals = pd.Series(imp.importances_mean, index=top_cols).sort_values(ascending=False)\n",
    "        top_imp = imp_vals.head(80).to_frame(\"permutation_importance\")\n",
    "    except Exception:\n",
    "        top_imp = corr_abs.head(args.pi_max_features).to_frame(\"proxy_importance\")\n",
    "\n",
    "    # Feature bars\n",
    "    save_top_features_bar(top_imp.iloc[:,0], os.path.join(out_dir, \"top_features_bar.png\"),\n",
    "                          \"Top Feature Importances (test-window)\")\n",
    "\n",
    "    # Import/tariff attribution\n",
    "    imp_imports = top_imp[top_imp.index.str.contains(\"Import__|tariff_|TariffWgt__\", case=False, regex=True)]\n",
    "    if imp_imports.empty:\n",
    "        it_cols = [c for c in X_test.columns if (\"Import__\" in c) or (\"tariff_\" in c.lower()) or (c.startswith(\"TariffWgt__\"))]\n",
    "        if it_cols:\n",
    "            sarima_resid_test = (y_test_level - sarima_pred_level).dropna()\n",
    "            proxy = pd.Series({c: abs(sarima_resid_test.corr(X_test[c].reindex(sarima_resid_test.index))) for c in it_cols})\n",
    "            imp_imports = proxy.sort_values(ascending=False).to_frame(\"proxy_importance\")\n",
    "\n",
    "    # 14) Save artifacts\n",
    "    level_metrics.to_csv(os.path.join(out_dir, \"level_metrics.csv\"))\n",
    "    mom_metrics.to_csv(os.path.join(out_dir, \"mom_metrics.csv\"))\n",
    "    yoy_metrics.to_csv(os.path.join(out_dir, \"yoy_metrics.csv\"))\n",
    "    qoq_metrics.to_csv(os.path.join(out_dir, \"qoq_saar_metrics.csv\"))\n",
    "    train_metrics.to_csv(os.path.join(out_dir, \"train_metrics.csv\"))\n",
    "    top_imp.to_csv(os.path.join(out_dir, \"top_feature_importances.csv\"))\n",
    "    imp_imports.head(30).to_csv(os.path.join(out_dir, \"top_import_tariff_categories.csv\"))\n",
    "\n",
    "    # Ensemble weights bar\n",
    "    w_series = pd.Series(weights).sort_values(ascending=True)\n",
    "    plt.figure(figsize=(8,4)); w_series.plot(kind=\"barh\"); plt.title(\"Ensemble Weights\"); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"ensemble_weights.png\"), dpi=150); plt.close()\n",
    "\n",
    "    with open(os.path.join(out_dir, \"summary.json\"), \"w\") as f:\n",
    "        json.dump({\n",
    "            \"xgb_present\": XGB_PRESENT,\n",
    "            \"best_sarima_order\": str((order, sorder)),\n",
    "            \"charts\": {\"level\": lvl_path, \"yoy\": yoy_path, \"mom\": mom_path, \"qoq\": qoq_path},\n",
    "            \"weights\": {k: float(v) for k,v in weights.items()},\n",
    "            \"calibration\": {\"a\": a_cal, \"b\": b_cal, \"sigma_train\": sigma_train}\n",
    "        }, f, indent=2)\n",
    "\n",
    "    # 15) Data audit\n",
    "    audit.update({\n",
    "        \"final_train_span\": [str(y_train.index.min().date()), str(y_train.index.max().date())],\n",
    "        \"final_test_span\":  [str(y_test.index.min().date()),  str(y_test.index.max().date())],\n",
    "        \"final_exog_numeric_count\": X.shape[1],\n",
    "        \"features_after_engineering_count\": X_ml.shape[1],\n",
    "        \"target_mode_for_ml\": \"MoM (reconstructed→Level)\" if use_mom else \"Level\"\n",
    "    })\n",
    "    with open(os.path.join(out_dir, \"data_used_summary.json\"), \"w\") as f:\n",
    "        json.dump(audit, f, indent=2)\n",
    "\n",
    "    # 16) Console summary\n",
    "    print(f\"\\n=== OUTPUTS written to {out_dir} ===\\n\")\n",
    "    print(level_metrics if not level_metrics.empty else \"No level metrics computed.\")\n",
    "    print(\"\\nMoM metrics:\\n\", mom_metrics if not mom_metrics.empty else \"N/A\")\n",
    "    print(\"\\nYoY metrics:\\n\", yoy_metrics if not yoy_metrics.empty else \"N/A\")\n",
    "    print(\"\\nQoQ SAAR metrics:\\n\", qoq_metrics if not qoq_metrics.empty else \"N/A\")\n",
    "    print(f\"\\nSpecial features in TRAIN: Import={audit['import_cols_in_train']}, Tariff={audit['tariff_cols_in_train']}, TariffWgt={audit['tariffwgt_cols_in_train']}\")\n",
    "    print(f\"Special features in TEST:  Import={audit['import_cols_in_test']},  Tariff={audit['tariff_cols_in_test']},  TariffWgt={audit['tariffwgt_cols_in_test']}\")\n",
    "    print(\"\\nTop import/tariff categories (by permutation or proxy importance):\")\n",
    "    print(imp_imports.head(10) if not imp_imports.empty else \"No import/tariff features available in this run.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    if \"-f\" in sys.argv:  # Jupyter kernel arg\n",
    "        sys.argv = [sys.argv[0]]\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ce6ebf-0347-47ee-8319-76945e91bcf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
